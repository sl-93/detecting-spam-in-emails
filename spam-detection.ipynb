{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMAocTPJ8Y6luEBqpZV/B8C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sl-93/detecting-spam-in-emails/blob/main/spam-detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5nxsZa3ZUtr"
      },
      "source": [
        "# Mount the google drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGPorybN-aHZ"
      },
      "source": [
        "# install nltk\r\n",
        "# NLTK: Natural Language tool kit\r\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUyMXhPG-eLs"
      },
      "source": [
        "# install gensim\r\n",
        "# Gensim is an open-source library for unsupervised topic modeling and natural language processing\r\n",
        "# Gensim is implemented in Python and Cython.\r\n",
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kue61W28ZYJo"
      },
      "source": [
        "# import key libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "import gensim\r\n",
        "import string\r\n",
        "import os\r\n",
        "string.punctuation\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "# Tensorflow\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout\r\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY996XXEbTja"
      },
      "source": [
        "import string\r\n",
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am4koyCBeYWF"
      },
      "source": [
        "# download stopwords\r\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnzbvNhOaWqf"
      },
      "source": [
        "# Load the data\r\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/spam/train.csv')\r\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/spam/test.csv')\r\n",
        "train_df = train_df.iloc[: , 0:2]\r\n",
        "train_df.dropna(inplace=True)\r\n",
        "test_df.dropna(inplace=True)\r\n",
        "y_train = train_df['Class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPcquEeWajup"
      },
      "source": [
        "# define a function to remove punctuations\r\n",
        "def remove_punc(message):\r\n",
        "    Test_punc_removed = [char for char in message if char not in string.punctuation]\r\n",
        "    Test_punc_removed_join = ''.join(Test_punc_removed)\r\n",
        "\r\n",
        "    return Test_punc_removed_join\r\n",
        "\r\n",
        "# remove punctuations from our datasets\r\n",
        "train_df['Text Without Punctuation'] = train_df['Text'].apply(remove_punc)\r\n",
        "test_df['Text Without Punctuation'] = test_df['Text'].apply(remove_punc)\r\n",
        "\r\n",
        "\r\n",
        "# Obtain additional stopwords from nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "stop_words.extend(['https'])\r\n",
        "\r\n",
        "# Remove stopwords and remove short words\r\n",
        "def preprocess(text):\r\n",
        "    result = []\r\n",
        "    for token in gensim.utils.simple_preprocess(text):\r\n",
        "        if token not in stop_words:\r\n",
        "            result.append(token)\r\n",
        "\r\n",
        "    return result\r\n",
        "\r\n",
        "# apply pre-processing to the text column\r\n",
        "train_df['Text Without Punc & Stopwords'] = train_df['Text Without Punctuation'].apply(preprocess)\r\n",
        "test_df['Text Without Punc & Stopwords'] = test_df['Text Without Punctuation'].apply(preprocess)\r\n",
        "\r\n",
        "# join the words into a string\r\n",
        "train_df['Text Without Punc & Stopwords Joined'] = train_df['Text Without Punc & Stopwords'].apply(lambda x: \" \".join(x))\r\n",
        "test_df['Text Without Punc & Stopwords Joined'] = test_df['Text Without Punc & Stopwords'].apply(lambda x: \" \".join(x))\r\n",
        "\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "# Obtain the maximum length of data in the document\r\n",
        "# This will be later used when word embeddings are generated\r\n",
        "maxlen = -1\r\n",
        "for doc in train_df['Text Without Punc & Stopwords Joined']:\r\n",
        "    tokens = nltk.word_tokenize(doc)\r\n",
        "    if(maxlen < len(tokens)):\r\n",
        "        maxlen = len(tokens)\r\n",
        "\r\n",
        "tweets_length = [ len(nltk.word_tokenize(x)) for x in train_df['Text Without Punc & Stopwords Joined'] ]\r\n",
        "\r\n",
        "# Obtain the total words present in the dataset\r\n",
        "list_of_words = []\r\n",
        "for i in train_df['Text Without Punc & Stopwords']:\r\n",
        "    for j in i:\r\n",
        "        list_of_words.append(j)\r\n",
        "\r\n",
        "# Obtain the total number of unique words\r\n",
        "total_words = len(list(set(list_of_words)))\r\n",
        "X_train = train_df['Text Without Punc & Stopwords']\r\n",
        "X_test = test_df['Text Without Punc & Stopwords']\r\n",
        "\r\n",
        "# Create a tokenizer to tokenize the words and create sequences of tokenized words\r\n",
        "tokenizer = Tokenizer(num_words = total_words)\r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "\r\n",
        "# Training data\r\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\r\n",
        "\r\n",
        "# Testing data\r\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\r\n",
        "\r\n",
        "# Add padding to training and testing\r\n",
        "padded_train = pad_sequences(train_sequences, maxlen = maxlen, padding = 'post', truncating = 'post')\r\n",
        "padded_test = pad_sequences(test_sequences, maxlen = maxlen, padding = 'post', truncating = 'post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOptBaar-5rJ"
      },
      "source": [
        "# Convert the data to categorical 2D representation\r\n",
        "y_train_cat = to_categorical(y_train, 2)\r\n",
        "label_encoder = LabelEncoder()\r\n",
        "y_train = label_encoder.fit_transform(y_train)\r\n",
        "clas = label_encoder.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05eXzq77apPB"
      },
      "source": [
        "# Sequential Model\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "# embedding layer\r\n",
        "model.add(Embedding(total_words, output_dim = 512))\r\n",
        "\r\n",
        "#Addding Bi-directional LSTM\r\n",
        "model.add(Bidirectional(tf.keras.layers.LSTM(64)))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "# Dense layers\r\n",
        "model.add(Dense(16, activation = 'relu'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(2,activation = 'softmax'))\r\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\r\n",
        "#model.save(\"my_model\")\r\n",
        "\r\n",
        "# train the model\r\n",
        "model.fit(padded_train, y_train_cat, batch_size = 64, validation_split = 0.2, epochs = 2)\r\n",
        "\r\n",
        "# It can be used to reconstruct the model identically.\r\n",
        "#reconstructed_model = keras.models.load_model(\"my_model\")\r\n",
        "\r\n",
        "# make prediction\r\n",
        "# Let's check:\r\n",
        "pred = model.predict(padded_test)\r\n",
        "\r\n",
        "prediction = []\r\n",
        "for i in pred:\r\n",
        "  prediction.append(clas[np.argmax(i)])\r\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}